---
title: "Part II - Ascertainig Multi-collinearity"
author: "Sanjeev Gadre"
date: "October 23, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE)
```

Loading the required libraries

```{r libraries, message=FALSE}
library(dplyr)
library(ggplot2)
library(ff)
library(ffbase)
library(biglm)

# Creating ffdir where ff objects can be stored and retrieved
ffdir = paste(getwd(), "/ffdir", sep = "")
if (!dir.exists(ffdir)){dir.create(ffdir)}

```

Loading utility functions

```{r utilitity-functions}
# save.ffdf(train, dir = "../RDA/traindir", overwrite = TRUE)
# save.ffdf(test, dir = "../RDA/testdir", overwrite = TRUE)
# 
# test = load.ffdf(dir = "../RDA/testdir")
#     test = test[["test"]]
#     open.ffdf(test)
    

```

### Functions for Mini-batch Gradient Descent with Linear Activation

```{r gradient-descent-functions-linear}
param.init = function (p.in, p.out){
  set.seed(1970)
 
  W = matrix(runif(p.in*p.out)*10, nrow = p.in)
  b = rep(0, p.out)
  
  param.cache = list("W" = W, "b" = b)

  return(param.cache)
}

fwd.prop.linear = function(A.in, param.cache){
  m = nrow(A.in)
  W = param.cache[["W"]]
  b = param.cache[["b"]]
  B = matrix(rep(b, m), nrow = m)             # Because R does not support "broadcast"
  
  A.out = A.in %*% W + b   

  return(A.out)
}

cost.fn.linear = function(Y, Y.hat, param.cache, lambda = 0){
  m = nrow(Y.hat)
  W = param.cache[["W"]]
  
  J = (1/(2*m))*(sum((Y.hat-Y)^2) + lambda*sum(W^2))
  
  return(J)
}

grad.cache.init = function(param.cache, acc = "none") {
  
  W = param.cache[["W"]]
  dW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
  db = 0
  grad.cache = list("dW" = dW, "db" = db)
  
  if (acc == "mome") {
    VdW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
    Vdb = 0

    grad.cache = append(grad.cache, list("VdW" = VdW, "Vdb" = Vdb))
  }

  if (acc == "rmsp") {
    SdW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
    Sdb = 0

    grad.cache = append(grad.cache, list("SdW" = SdW, "Sdb" = Sdb))
  }

  if (acc == "adam") {
    VdW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
    Vdb = 0
    SdW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
    Sdb = 0

    grad.cache = append(grad.cache, list("VdW" = VdW, "Vdb" = Vdb, "SdW" = SdW, "Sdb" = Sdb))
  }
  
  return(grad.cache)
}

# lambda value needs to be passed if cost regularization is used
# beta1 and beta2 values need to be passed if accelaration is desired
  # beta1 for "Momentum", beta2 for "RMSprop" and both for "Adam"

bwd.prop.linear = function(X, Y, A.out, param.cache, grad.cache, lambda = 0, beta1 = 0.9, beta2 = 0.999){
  m = nrow(A.out)
  W = param.cache[["W"]]
  VdW = grad.cache[["VdW"]];    Vdb = grad.cache[["Vdb"]]
  SdW = grad.cache[["SdW"]];    Sdb = grad.cache[["Sdb"]]
  
  dZ = A.out - Y
  dW = (1/m)*(t(X) %*% dZ) + (lambda/m)*W
  db = (1/m)*apply(dZ, 2, sum)
  
  grad.cache[["dW"]] = dW
  grad.cache[["db"]] = db
  
  if (!is.null(VdW) && is.null(SdW)) {                    # Gradient Descent with momentum
    VdW = beta1*VdW + (1-beta1)*dW
    Vdb = beta1*Vdb + (1-beta1)*db
    grad.cache[["VdW"]] = VdW ;   grad.cache[["Vdb"]] = Vdb
  }
  
  if (is.null(VdW) && !is.null(SdW)) {                    # Gradient Descent with RMSprop
    SdW = beta2*SdW + (1-beta2)*dW^2
    Sdb = beta2*Sdb + (1-beta2)*db^2
    grad.cache[["SdW"]] = SdW ;   grad.cache[["Sdb"]] = Sdb
  }
  
  if (!is.null(VdW) && !is.null(SdW)) {                   # Gradient Descent with Adam
    VdW = grad.cache[["VdW"]]; Vdb = grad.cache[["Vdb"]]
    VdW = beta1*VdW + (1-beta1)*dW
    Vdb = beta1*Vdb + (1-beta1)*db
    grad.cache[["VdW"]] = VdW ;   grad.cache[["Vdb"]] = Vdb
    
    SdW = grad.cache[["SdW"]]; Sdb = grad.cache[["Sdb"]]
    SdW = beta2*SdW + (1-beta2)*dW^2
    Sdb = beta2*Sdb + (1-beta2)*db^2
    grad.cache[["SdW"]] = SdW ;   grad.cache[["Sdb"]] = Sdb
  }
  
  return(grad.cache)
}

param.update = function(alpha, grad.cache, param.cache, beta1 = 0.9, beta2 = 0.999, t = 10^8){
  
  W = param.cache[["W"]];    b = param.cache[["b"]]
  dW = grad.cache[["dW"]];    db = grad.cache[["db"]]
  VdW = grad.cache[["VdW"]];  Vdb = grad.cache[["Vdb"]]
  SdW = grad.cache[["SdW"]];  Sdb = grad.cache[["Sdb"]]
  
  if (is.null(VdW) && is.null(SdW)) {                     # Standard Gradient Descent 
    W = W - alpha*dW
    b = b - alpha*db
    
    param.cache[["W"]] = W;  param.cache[["b"]] = b
  }
  
  if (!is.null(VdW) && is.null(SdW)) {                    # Gradient Descent with momentum
    W = W - alpha*VdW
    b = b - alpha*Vdb
    
    param.cache[["W"]] = W;  param.cache[["b"]] = b
  }
  
  if (is.null(VdW) && !is.null(SdW)) {                    # Gradient Descent with RMSprop
    W = W - alpha*dW/(sqrt(SdW) + 10^-8)                  # 10^-8 added to avoid division by zero
    b = b - alpha*db/(sqrt(Sdb) + 10^-8)
    
    param.cache[["W"]] = W;  param.cache[["b"]] = b
  }
  
  if (!is.null(VdW) && !is.null(SdW)) {                   # Gradient Descent with Adam
    VdW = VdW/(1-beta1^t);    Vdb = Vdb/(1-beta1^1)
    SdW = SdW/(1-beta2^t);    Sdb = Sdb/(1-beta2^1)       # Correcting for running count of mini-batch
    
    
    W = W - alpha*VdW/(sqrt(SdW) + 10^-8)                 
    b = b - alpha*Vdb/(sqrt(Sdb) + 10^-8)
    
    param.cache[["W"]] = W;  param.cache[["b"]] = b
  }
  
  return(param.cache)
}

```

### Getting data

```{r get-data}
train = load.ffdf(dir = "../RDA/traindir")
train = train[["train"]]
open.ffdf(train)
#ffload(paste(ffdir, "/train.y", sep = ""))


```

```{r temp}
feat = 1

# To find the VIF, we regress every independent variable on all other independent variables.
# Therefore, for these regressions, the number of features in input layer will 255

p.in = ncol(train) - 1      # number of features for the input
m = nrow(train)             # number of training examples
alpha = 10^-2               # learning rate
epochs = 10^1               # number of epochs or iterations
lambda = 0                  # regularization penalty
acc = "none"                # gradient descent accelaration algorithm
beta1 = 0.9                 # gradient descent accelaration algorithm parameter
beta2 = 0.999               # gradient descent accelaration algorithm parameter
mini.bat.sz = 64            # mini-batch size (=1 for stochastic, =m for gradient descent, >1 <m for mini-batch)
t = 1                       # running count of mini-batches across epochs used for Adam accelaration


param.cache = param.init(p.in, 1)                           # initializing the weights
grad.cache = grad.cache.init(param.cache, acc)              # initializing the gradients
cost.cache = list("cost" = NULL, "iter" = NULL)             # list to save costs periodically

vif = rep(0, (p.in+1))      # Vector to store the VIF for individual features
```

```{r calc-vif}
feat = 1

# To find the VIF, we regress every independent variable on all other independent variables.
# Therefore, for these regressions, the number of features in input layer will 255

p.in = ncol(train) - 1      # number of features for the input
m = nrow(train)             # number of training examples
alpha = 10^-2               # learning rate
epochs = 10^1               # number of epochs or iterations
lambda = 0                  # regularization penalty
acc = "none"                # gradient descent accelaration algorithm
beta1 = 0.9                 # gradient descent accelaration algorithm parameter
beta2 = 0.999               # gradient descent accelaration algorithm parameter
mini.bat.sz = 64            # mini-batch size (=1 for stochastic, =m for gradient descent, >1 <m for mini-batch)
t = 1                       # running count of mini-batches across epochs used for Adam accelaration
vif = rep(0, (ncol(train))) # Vector to store the VIF for individual features

for (feat in 1:ncol(train)) {
  # Code for linear regression of feat over all other variables
}


param.cache = param.init(p.in, 1)                           # initializing the weights
grad.cache = grad.cache.init(param.cache, acc)              # initializing the gradients
cost.cache = list("cost" = NULL, "iter" = NULL)             # list to save costs periodically

timestamp()

if (m < mini.bat.sz) {                                      # Sanity check
  mini.bat.nos = m
} else {
  mini.bat.nos = m %/% mini.bat.sz                          # Calculating the number of full mini-batches
}

for (i in 1:epochs) {
  set.seed(i)
  indx = sample(1:m, m)
  
  for (j in 1:mini.bat.nos) {
    end = j*mini.bat.sz
    beg = end - mini.bat.sz + 1
    minindex = indx[beg:end]
    
    X.mini = train[minindex, -feat] %>% as.matrix()
    Y.mini = train[minindex, feat] %>% as.numeric()
    
    if (!is.matrix(X.mini)) {                                                    # Sanity Check
      stop("X.mini is not a matrix. Fatal Error. Quitting!")
    }
    if (!is.numeric(Y.mini)) {
      stop("Y.mini is not a numeric vector. Fatal Error. Quitting!")
    }
    
    A.out = fwd.prop.linear(X.mini, param.cache)                                 # Forward Propagation
    
    cost = cost.fn.linear(Y.mini, A.out, param.cache, lambda)                    # Calculate the cost function
    cost.cache$cost = append(cost.cache$cost, cost)
    cost.cache$iter = append(cost.cache$iter, t)
    
    grad.cache = bwd.prop.linear(X.mini, Y.mini, A.out, param.cache, grad.cache, 
                                 lambda, beta1, beta2)                           # Backward Propagation
    
    param.cache = param.update(alpha, grad.cache, param.cache,
                               beta1, beta2, t)                                 # Updating the weights
    
    t = t + 1
  }
  
  if (m %% mini.bat.sz != 0) {          # Accounting for the non-full-sized mini-batch
    beg = mini.bat.nos*mini.bat.sz + 1
    end = m
    minindex = indx[beg:end]
    
    X.mini = train[minindex, -feat] %>% as.matrix()
    Y.mini = train[minindex, feat] %>% as.numeric()
    
    A.out = fwd.prop.linear(X.mini, param.cache)                                 # Forward Propagation
    
    cost = cost.fn.linear(Y.mini, A.out, param.cache, lambda)                    # Calculate the cost function
    cost.cache$cost = append(cost.cache$cost, cost)
    cost.cache$iter = append(cost.cache$iter, t)
    
    grad.cache = bwd.prop.linear(X.mini, Y.mini, A.out, param.cache, grad.cache, 
                                 lambda, beta1, beta2)                           # Backward Propagation
    
    param.cache = param.update(alpha, grad.cache, param.cache,
                               beta1, beta2, t)                                 # Updating the weights
    
    t = t + 1
  }
  
}

plot(x = cost.cache$iter, y = cost.cache$cost, type = "l", xlab = "Number of Minibatches", ylab = "Cost")

timestamp()

preds = fwd.prop.linear(train[, -feat] %>% as.matrix(), param.cache)

RSS = (preds - train[, feat])^2 %>% sum()
TSS = (mean(train[, feat]) - train[, feat])^2 %>% sum()
Rsq = 1 - RSS/TSS
vif[feat] = 1/(1-Rsq)

print(paste("The R-squared for the fit = ", round(100*Rsq, 2), "%", sep = ""))

```

```{r temp-2}
preds = fwd.prop.linear(train[, -feat] %>% as.matrix(), param.cache)

RSS = (preds - train[, feat])^2 %>% sum()
TSS = (mean(train[, feat]) - train[, feat])^2 %>% sum()
Rsq = 1 - RSS/TSS

print(paste("The R-squared for the fit = ", round(100*Rsq, 2), "%", sep = ""))


```

### Calculating VIF

```{r vif}
p = ncol(train)
vif = rep(0, p)

for (k in 1:p) {
  indx = setdiff(1:p, k)
  f = paste(colnames(train)[k], "~", colnames(train)[indx[1]], sep = "")
  for (i in indx[-1]) {
    f = paste(f, "+", colnames(train)[i], sep = "")
  }
  
  f = as.formula(f)
  model = bigglm.ffdf(f, data = train)
  rsq = summary(model)$rsq
  vif[k] = 1/(1-rsq)
  
  print(paste("Finished Processing feature", k))
}

saveRDS(vif, "../RDA/vif.Rda")
```


<!-- ```{r} -->
<!-- m = nrow(train) -->
<!-- k = 51000 -->
<!-- n = ceiling(m/k) -->
<!-- p = ncol(train) -->
<!-- cif = rep(0, p) -->
<!-- feat = 1:2 -->
<!-- #feat = sample(1:p, 10) -->

<!-- timestamp() -->
<!-- for (f in feat) { -->
<!--   for (i in 1:n) { -->
<!--     indx = sample(1:m, k) -->
<!--     #print(indx[1]) -->
<!--     dat = data.frame(train[indx, -f], target = train[indx, f]) -->
<!--     model = lm(target~., dat) -->
<!--     cif[f] = vif[f] + summary(model)$r.squared -->
<!--   } -->

<!--   cif[f] = cif[f]/n -->
<!--   cif[f] = 1/(1-cif[f]) -->
<!--   print(paste("Finished for feature", f)) -->
<!-- } -->
<!-- timestamp() -->

<!-- ``` -->


<!-- ```{r incremental-linear-regression} -->
<!-- p = ncol(train) -->
<!-- m = nrow(train) -->
<!-- mini.bat.sz = 10000 -->
<!-- mini.bat.nos = m %/% mini.bat.sz -->
<!-- vif = rep(0, p) -->


<!-- timestamp() -->
<!-- for (feat in 1:p) { -->
<!--   dat = data.frame(train[1:mini.bat.size, -feat], target = train[1:mini.bat.size, feat]) -->
<!--   model = lm(target~., dat) -->

<!--   for(j in 2:bat.nos){ -->
<!--     end = j*mini.bat.sz -->
<!--     beg = end - mini.bat.sz + 1 -->

<!--     dat = data.frame(train[beg:end, -feat], target = train[beg:end, feat]) -->
<!--     model = update(model, .~., dat) -->

<!--     #print(paste("Fitted model for feat = ", feat, ", and mini-batch no = ", j)) -->
<!--   } -->

<!--   if (m %% mini.bat.sz != 0) {          # Accounting for the non-full-sized mini-batch -->
<!--     beg = mini.bat.nos*mini.bat.sz + 1 -->
<!--     end = m -->

<!--     dat = data.frame(train[beg:end, -feat], target = train[beg:end, feat]) -->
<!--     model = update(model, .~., dat) -->

<!--     print(paste("Fitted model for feat = ", feat, ", for non-full mini-batch")) -->

<!--   } -->

<!--   vif[feat] = 1/(1-summary(model)$r.squared) -->

<!-- } -->

<!-- timestamp() -->

<!-- ``` -->

<!-- ```{r} -->
<!-- beg = 1 -->
<!-- end = 20000 -->
<!-- dat = data.frame(train[beg:end, -1], target = train[beg:end, 1]) -->
<!-- model.full = lm(target~., dat) -->
<!-- summary(model.full)$r.squared -->

<!-- # beg = 20001 -->
<!-- # end = 40000 -->
<!-- # dat = data.frame(train[1:m, -1], target = train[1:m, 1]) -->
<!-- # model.full = update(model.full, dat) -->
<!-- # summary(model.full)$r.squared -->

<!-- m = 20000 -->
<!-- mini.bat.sz = 2000 -->
<!-- mini.bat.nos = m %/% mini.bat.sz -->

<!-- indx = sample(1:m, m) -->
<!-- end = 1*mini.bat.sz -->
<!-- beg = end - mini.bat.sz + 1 -->
<!-- minindex = indx[beg:end] -->
<!-- dat = data.frame(train[minindex, -1], target = train[minindex, 1]) -->

<!-- model = lm(target~., dat) -->

<!-- for (i in 1:2) { -->
<!--   for (j in 1:mini.bat.nos) { -->
<!--       end = j*mini.bat.sz -->
<!--       beg = end - mini.bat.sz + 1 -->
<!--       minindex = indx[beg:end] -->
<!--       dat = data.frame(train[minindex, -1], target = train[minindex, 1]) -->

<!--       model = update(model, dat, evaluate = TRUE) -->

<!--       if (j %% 1 == 0) {print(summary(model)$r.squared)} -->
<!--   } -->
<!--   #print(summary(model)$r.squared) -->
<!-- } -->


<!-- ``` -->

