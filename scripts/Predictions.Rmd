---
title: "Part III - Predictions"
author: "Sanjeev Gadre"
date: "November 15, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE)
```

Loading the required libraries

```{r libraries, message=FALSE}
library(dplyr)
library(ggplot2)
library(ff)
library(ffbase)
#library(biglm)
library(ROCR)
#library(glmnet)
#library(e1071)
library(MASS)

# Creating ffdir where ff objects can be stored and retrieved
ffdir = paste(getwd(), "/ffdir", sep = "")
if (!dir.exists(ffdir)){dir.create(ffdir)}

```

### Getting Data

1.  We load the necessary datasets processed and saved in the earlier phase.

```{r get-data, message=FALSE}
train = load.ffdf(dir = "../RDA/traindir")
    train = train[["train"]]
    open.ffdf(train)
test = load.ffdf(dir = "../RDA/testdir")
    test = test[["test"]]
    open.ffdf(test)
ffload(paste(ffdir, "/test.id.val"))

```

### Setup

1.  Get the setup for making and storing predictions.

```{r setup}
subsets = unique(train$wheezy.copper.turtle.magic[])
out = ffdf(id = test.id.val, key = test$wheezy.copper.turtle.magic, target = ff(rep(0, nrow(test))))

comps = 20:55

```

### QDA Regression Models

1.  Fitting QDA Models to every subset of *train* dataset. 
2.  We use the *train-val* strategy to determine the number of principal components to include in the final model for that subset.
3.  Based on earlier analysis, we cross validate the number of principal compnents over the range 20-55.
4.  We then predict the probability for class=1 for all *test* dataset observations in the particular subset.

```{r multiple-qda-models, warning=FALSE}
for (subset in subsets) {
  dat = train[train$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame()
  y = dat$target
  dat = subset(dat, select = -c(wheezy.copper.turtle.magic, target)) 

  set.seed(subset)
  indx = sample(1:nrow(dat), 0.2*nrow(dat))
  
  pca.out = prcomp(dat)
  dat = pca.out$x
  
  auc = rep(0, length(comps))   # Store the val-set auc for models with candidate number of principal components
  j = 1
  for (comp in comps) {
    set.seed(comp)
    model = qda(dat[-indx, 1:comp], y[-indx])
    
    prob = predict(model, newdata = dat[indx, 1:comp])$posterior[, "1"]
    pred = prediction(prob, y[indx])
    auc[j] = performance(pred, "auc")@y.values %>% unlist()
    j = j + 1
  }
  
  best.comp = which.max(auc) %>% comps[.]
  
  set.seed(best.comp)
  model = qda(dat[, 1:best.comp], y)
  
  newdat = test[test$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame()
  newdat = subset(newdat, select = -wheezy.copper.turtle.magic)
  newdat = prcomp(newdat)
  newdat = newdat$x[, 1:best.comp]
  prob = predict(model, newdata = newdat)$posterior[, "1"]
  out$target[out$key == subset] = ff(prob)

} 

```

### Finalizing the output

```{r preds-out}
out = subset.ffdf(out, select = -key)
out = data.frame(out)

write.csv(out, file = "../data/out.csv", quote = FALSE, row.names = FALSE)

```

