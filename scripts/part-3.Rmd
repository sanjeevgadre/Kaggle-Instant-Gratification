---
title: "Part III - Logistic Regression"
author: "Sanjeev Gadre"
date: "October 23, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE)
```

Loading the required libraries

```{r libraries, message=FALSE}
library(dplyr)
library(ggplot2)
library(ff)
library(ffbase)

# Creating ffdir where ff objects can be stored and retrieved
ffdir = paste(getwd(), "/ffdir", sep = "")
if (!dir.exists(ffdir)){dir.create(ffdir)}

```


<!-- Loading utility functions -->

<!-- ```{r utilitity-functions} -->
<!-- save.ffdf(train, dir = "../RDA/traindir", overwrite = TRUE) -->
<!-- save.ffdf(test, dir = "../RDA/testdir", overwrite = TRUE) -->

<!-- test = load.ffdf(dir = "../RDA/testdir") -->
<!--     test = test[["test"]] -->
<!--     open.ffdf(test) -->


<!-- ``` -->

### Functions for Mini-batch Gradient Descent

```{r gradient-descent-functions}
param.init = function (p.in, p.out){
  set.seed(1970)
 
  W = matrix(runif(p.in*p.out)*10, nrow = p.in)
  b = rep(0, p.out)
  
  param.cache = list("W" = W, "b" = b)

  return(param.cache)
}

fwd.prop.sigmoid = function(A.in, param.cache){
  epsilon = 10^-7
  m = nrow(A.in)
  W = param.cache[["W"]]
  b = param.cache[["b"]]
  B = matrix(rep(b, m), nrow = m)             # Because R does not support "broadcast"
  
  Z = A.in %*% W + B   
  A.out = 1/(1+exp(-Z))
  # Scaling A.out from [0,1] to [epsilon, 1-epsilon] to avoid NaN's in cost calculation
  A.out = epsilon + A.out*(1-2*epsilon)   
  
  return(A.out)
}

# lambda value needs to be passed if cost regularization is desired

cost.fn.sigmoid = function(Y, Y.hat, param.cache, lambda = 0){
  m = nrow(Y.hat)
  W = param.cache[["W"]]
  
  J = (-1/m)*sum(Y*log(Y.hat) + (1-Y)*log(1-Y.hat)) + (lambda/(2*m))*sum(W^2)
  return(J)
}

grad.cache.init = function(param.cache, acc = "none") {
  
  W = param.cache[["W"]]
  dW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
  db = 0
  grad.cache = list("dW" = dW, "db" = db)
  
  if (acc == "mome") {
    VdW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
    Vdb = 0

    grad.cache = append(grad.cache, list("VdW" = VdW, "Vdb" = Vdb))
  }

  if (acc == "rmsp") {
    SdW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
    Sdb = 0

    grad.cache = append(grad.cache, list("SdW" = SdW, "Sdb" = Sdb))
  }

  if (acc == "adam") {
    VdW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
    Vdb = 0
    SdW = matrix(rep(0, nrow(W)*ncol(W)), nrow = nrow(W))
    Sdb = 0

    grad.cache = append(grad.cache, list("VdW" = VdW, "Vdb" = Vdb, "SdW" = SdW, "Sdb" = Sdb))
  }
  
  return(grad.cache)
}

# lambda value needs to be passed if cost regularization is used
# beta1 and beta2 values need to be passed if accelaration is desired
  # beta1 for "Momentum", beta2 for "RMSprop" and both for "Adam"


bwd.prop.sigmoid = function(X, Y, A.out, param.cache, grad.cache, lambda = 0, beta1 = 0.9, beta2 = 0.999){
  m = nrow(A.out)
  W = param.cache[["W"]]
  VdW = grad.cache[["VdW"]];    Vdb = grad.cache[["Vdb"]]
  SdW = grad.cache[["SdW"]];    Sdb = grad.cache[["Sdb"]]
  
  dZ = A.out - Y
  dW = (1/m)*(t(X) %*% dZ) + (lambda/m)*W
  db = (1/m)*apply(dZ, 2, sum)
  
  grad.cache[["dW"]] = dW
  grad.cache[["db"]] = db
  
  if (!is.null(VdW) && is.null(SdW)) {                    # Gradient Descent with momentum
    VdW = beta1*VdW + (1-beta1)*dW
    Vdb = beta1*Vdb + (1-beta1)*db
    grad.cache[["VdW"]] = VdW ;   grad.cache[["Vdb"]] = Vdb
  }
  
  if (is.null(VdW) && !is.null(SdW)) {                    # Gradient Descent with RMSprop
    SdW = beta2*SdW + (1-beta2)*dW^2
    Sdb = beta2*Sdb + (1-beta2)*db^2
    grad.cache[["SdW"]] = SdW ;   grad.cache[["Sdb"]] = Sdb
  }
  
  if (!is.null(VdW) && !is.null(SdW)) {                   # Gradient Descent with Adam
    VdW = grad.cache[["VdW"]]; Vdb = grad.cache[["Vdb"]]
    VdW = beta1*VdW + (1-beta1)*dW
    Vdb = beta1*Vdb + (1-beta1)*db
    grad.cache[["VdW"]] = VdW ;   grad.cache[["Vdb"]] = Vdb
    
    SdW = grad.cache[["SdW"]]; Sdb = grad.cache[["Sdb"]]
    SdW = beta2*SdW + (1-beta2)*dW^2
    Sdb = beta2*Sdb + (1-beta2)*db^2
    grad.cache[["SdW"]] = SdW ;   grad.cache[["Sdb"]] = Sdb
  }
  
  return(grad.cache)
}

param.update = function(alpha, grad.cache, param.cache, beta1 = 0.9, beta2 = 0.999, t = 10^8){
  
  W = param.cache[["W"]];    b = param.cache[["b"]]
  dW = grad.cache[["dW"]];    db = grad.cache[["db"]]
  VdW = grad.cache[["VdW"]];  Vdb = grad.cache[["Vdb"]]
  SdW = grad.cache[["SdW"]];  Sdb = grad.cache[["Sdb"]]
  
  if (is.null(VdW) && is.null(SdW)) {                     # Standard Gradient Descent 
    W = W - alpha*dW
    b = b - alpha*db
    
    param.cache[["W"]] = W;  param.cache[["b"]] = b
  }
  
  if (!is.null(VdW) && is.null(SdW)) {                    # Gradient Descent with momentum
    W = W - alpha*VdW
    b = b - alpha*Vdb
    
    param.cache[["W"]] = W;  param.cache[["b"]] = b
  }
  
  if (is.null(VdW) && !is.null(SdW)) {                    # Gradient Descent with RMSprop
    W = W - alpha*dW/(sqrt(SdW) + 10^-8)                  # 10^-8 added to avoid division by zero
    b = b - alpha*db/(sqrt(Sdb) + 10^-8)
    
    param.cache[["W"]] = W;  param.cache[["b"]] = b
  }
  
  if (!is.null(VdW) && !is.null(SdW)) {                   # Gradient Descent with Adam
    VdW = VdW/(1-beta1^t);    Vdb = Vdb/(1-beta1^1)
    SdW = SdW/(1-beta2^t);    Sdb = Sdb/(1-beta2^1)       # Correcting for running count of mini-batch
    
    
    W = W - alpha*VdW/(sqrt(SdW) + 10^-8)                 
    b = b - alpha*Vdb/(sqrt(Sdb) + 10^-8)
    
    param.cache[["W"]] = W;  param.cache[["b"]] = b
  }
  
  return(param.cache)
}

```

### Getting data

```{r get-data}
train = load.ffdf(dir = "../RDA/traindir")
train = train[["train"]]
open.ffdf(train)
ffload(paste(ffdir, "/train.y", sep = ""))

```

### Train/Dev/Test

```{r train-dev-test}
# We are partioning the train datasets into 3: dev (20,000), test (0), train (remaining)

set.seed(1970)
m = nrow(train)
indx = split(sample(1:m, m), f = c(rep("dev", 20000), rep("train", m-20000)))

```

```{r logistic-regression}

p.in = ncol(train)          # number of features for the input
m = length(indx$train)      # number of training examples, in this case the train subset of train
alpha = 10^-5               # learning rate
epochs = 10^0               # number of epochs or iterations
lambda = 0                  # regularization penalty
acc = "none"                # gradient descent accelaration algorithm
beta1 = 0.9                 # gradient descent accelaration algorithm parameter
beta2 = 0.999               # gradient descent accelaration algorithm parameter
mini.bat.sz = 064           # mini-batch size (=1 for stochastic, =m for gradient descent, >1 <m for mini-batch)
t = 1                       # running count of mini-batches across epochs used for Adam accelaration


param.cache = param.init(p.in, 1)                           # initializing the weights
grad.cache = grad.cache.init(param.cache, acc)              # initializing the gradients
cost.cache = list("cost" = NULL, "iter" = NULL)             # list to save costs periodically

if (m < mini.bat.sz) {                                      # Sanity check
  mini.bat.nos = m
} else {
  mini.bat.nos = m %/% mini.bat.sz                          # Calculating the number of full mini-batches
}

for (i in 1:epochs) {
  set.seed(i)
  e.indx = sample(1:m, m)
  
  for (j in 1:mini.bat.nos) {
    end = j*mini.bat.sz
    beg = end - mini.bat.sz + 1
    minindex = indx$train[e.indx[beg:end]]

    X.mini = train[minindex, ] %>% as.matrix()             # Forming the mini-batch
    Y.mini = train.y[minindex] %>% as.numeric()
    
    if (!is.matrix(X.mini)) {                               # Sanity Check
      stop("X.mini is not a matrix. Fatal Error. Quitting!")
    }
    if (!is.numeric(Y.mini)) {
      stop("Y.mini is not a numeric vector. Fatal Error. Quitting!")
    }
    
    A.out = fwd.prop.sigmoid(X.mini, param.cache)                                 # Forward Propagation
    
    cost = cost.fn.sigmoid(Y.mini, A.out, param.cache, lambda)                    # Calculate the cost function
    cost.cache$cost = append(cost.cache$cost, cost)
    cost.cache$iter = append(cost.cache$iter, t)
    
    grad.cache = bwd.prop.sigmoid(X.mini, Y.mini, A.out, param.cache, grad.cache, 
                                 lambda, beta1, beta2)                           # Backward Propagation
    
    param.cache = param.update(alpha, grad.cache, param.cache,
                                beta1, beta2, t)                                 # Updating the weights
    
    t = t + 1
  }
  
  if (m %% mini.bat.sz != 0) {          # Accounting for the non-full-sized mini-batch
    beg = mini.bat.nos*mini.bat.sz + 1
    end = m
    minindex = indx$train[e.indx[beg:end]]

    X.mini = train[minindex, ] %>% as.matrix()             # Forming the mini-batch
    Y.mini = train.y[minindex] %>% as.numeric()
    
    if (!is.matrix(X.mini)) {                               # Sanity Check
      stop("X.mini is not a matrix. Fatal Error. Quitting!")
    }
    if (!is.numeric(Y.mini)) {
      stop("Y.mini is not a numeric vector. Fatal Error. Quitting!")
    }
    
    A.out = fwd.prop.sigmoid(X.mini, param.cache)                                 # Forward Propagation
    
    cost = cost.fn.sigmoid(Y.mini, A.out, param.cache, lambda)                    # Calculate the cost function
    cost.cache$cost = append(cost.cache$cost, cost)
    cost.cache$iter = append(cost.cache$iter, t)
    
    grad.cache = bwd.prop.sigmoid(X.mini, Y.mini, A.out, param.cache, grad.cache, 
                                 lambda, beta1, beta2)                           # Backward Propagation
    
    param.cache = param.update(alpha, grad.cache, param.cache,
                                beta1, beta2, t)                                 # Updating the weights
    
    t = t + 1
  }
  
}

plot(x = cost.cache$iter, y = cost.cache$cost, type = "l", xlab = "Number of Minibatches", ylab = "Cost")
```

### Calculating the Dev Set Error

```{r d}
probs = fwd.prop.sigmoid(train[indx$dev, ] %>% as.matrix(), param.cache)
preds = ifelse(probs < 0.5, 0, 1)

#table(preds, train.y[indx$dev]) %>% prop.table()

print(paste("The prediction accuracy on the dev set is", round(mean(preds == train.y[indx$dev]), 4), "%"))
```

```{r}
for (i in 1:25){
  indx = sample(1:nrow(train), 2500)

  X.sample = train[indx, ]
  Y.sample = train.y[indx]
  
  dat = data.frame(train[indx,], target = train.y[indx])
  
  model = prcomp(X.sample)
  var.xplnd = model$sdev^2/sum(model$sdev^2)
  cum.var.xplnd = cumsum(var.xplnd)
  
  print(which(cum.var.xplnd >= 0.85))
}

```

