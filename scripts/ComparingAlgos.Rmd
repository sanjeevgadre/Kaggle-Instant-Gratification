---
title: "Part II - Comparing Learning Models"
author: "Sanjeev Gadre"
date: "November 13, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE)
```

Loading the required libraries

```{r libraries, message=FALSE}
library(dplyr)
library(ggplot2)
library(ff)
library(ffbase)
library(biglm)
library(ROCR)
library(glmnet)
library(e1071)
library(MASS)

# Creating ffdir where ff objects can be stored and retrieved
ffdir = paste(getwd(), "/ffdir", sep = "")
if (!dir.exists(ffdir)){dir.create(ffdir)}

```

### Getting Data

1.  We load the necessary datasets processed and saved in the earlier phase.

```{r get-data, message=FALSE}
train = load.ffdf(dir = "../RDA/traindir")
    train = train[["train"]]
    open.ffdf(train)
test = load.ffdf(dir = "../RDA/testdir")
    test = test[["test"]]
    open.ffdf(test)
ffload(paste(ffdir, "/test.id.val"))

```

### Setting up for Models
1.  We will compare the performance of 3 learning models using **AUC** as the metric - Penalised Logistic Regression, Naive Bayes and Quadratic Discriminant Analysis.
2.  For all the three models, we will build a separate learning model for each of the underlying subsets in the *train* dataset. We will also use PCA to reduce the dimensionality before fitting the model.
3.  We compare the learning models for randomly chosen 50 subsets.

```{r setup-for-models}
subsets = unique(train$wheezy.copper.turtle.magic[])
set.seed(1947)
subsets = sample(1:length(subsets), 50)

# Matrix to store the performace of the different learning methods for each subset
model.perf = matrix(rep(0, 4*length(subsets)), ncol = 4)
colnames(model.perf) = c("subset", "LR", "NB", "QDA")
model.perf[, "subset"] = subsets

```

### Penalised Logistic Regression Models

1.  We develop penalised logistic regression models for the randomly chosen 50 *train* data subsets. 
2.  We use the *train-val-test* strategy to:
  a.  determine the ideal number of principal components to use in building the model
    iii.  PCA will deliver 255 principal components and we choose 25 values of principal components over the 
          range  10-235 for the validation set.
  b.  estimate the likely test Area Under the Curve metric
3.  One important fact to bear in mind is that when we perform a PCA, we transform the data from one feature subspace to another. We need to always make sure that the *train* and *test* datasets are from the **same** feature subspace. Therefore, when applyin the PCA to a data subset, it is important to include **both** the relevant *train* and *test* datasets.

```{r multiple-penalised-logit-models, warning=FALSE}
comps = seq(10, 235, length.out = 25) %>% floor()
xval.comps = matrix(rep(0, 2*length(subsets)), ncol = 2)
colnames(xval.comps) = c("subset", "X-val number of prinicpal Components")
xval.comps[, "subset"] = subsets
test.auc = 0
rows = 0

for (subset in subsets) {
  dat = train[train$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame(.)
  y = dat$target
  dat = subset(dat, select = -c(target, wheezy.copper.turtle.magic))
  m = nrow(dat)
  # Combining peer subsets from the train and test datasets
  dat = test[test$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame(.) %>% 
    subset(., select = -wheezy.copper.turtle.magic) %>% rbind(dat, .) 
  
  pca.out = prcomp(dat) 
  dat = pca.out$x[1:m, ]

  set.seed(subset)
  indx = split(sample(1:nrow(dat)), f = c(rep("train", 6), rep("val", 2), rep("test", 2)))
  
  auc = rep(0, length(comps))   # Store the val-set auc for models with candidate number of principal components
  j = 1
  for (comp in comps) {
    set.seed(comp)
    cv.out = cv.glmnet(dat[indx$train, 1:comp], y[indx$train], family = "binomial", nfolds = 5)
    best.lambda = cv.out$lambda.min
    model = glmnet(dat[indx$train, 1:comp], y[indx$train], family = "binomial", lambda = best.lambda)
    
    prob = predict(model, newx = dat[indx$val, 1:comp], type = "response")
    pred = prediction(prob, y[indx$val])
    auc[j] = performance(pred, "auc")@y.values %>% unlist()
    j = j + 1
  }
  
  best.comp = which.max(auc) %>% comps[.]
  xval.comps[xval.comps[, "subset"] == subset, "X-val number of prinicpal Components"] = best.comp

  set.seed(best.comp)
  cv.out = cv.glmnet(dat[-indx$test, 1:best.comp], y[-indx$test], family = "binomial", nfolds = 5)
  best.lambda = cv.out$lambda.min
  
  model = glmnet(dat[-indx$test, 1:best.comp], y[-indx$test], family = "binomial", lambda = best.lambda)
  prob = predict(model, newx = dat[indx$test, 1:best.comp], type = "response")
  
  pred = prediction(prob, y[indx$test])
  perf = performance(pred, "auc")
  perf = perf@y.values %>% unlist() %>% round(., 4)

  model.perf[model.perf[, "subset"] == subset, "LR"] = perf

  test.auc = test.auc + nrow(dat)*perf
  rows = rows + nrow(dat)
} 

writeLines("")
print(paste("The weighted estimated test AUC for the 50 subsets of train =", test.auc/rows))
print("The number of x-validated number of principal components used to build the 50 models are")
table(xval.comps[, "X-val number of prinicpal Components"])

```

*Observations*
1.  The Penalised Logistic Regression models deliver a reasonable (0.7854) weighted estimated test AUC using a sample of 50 subsets of the *train* dataset.
2.  For these 50 subsets, the cross-validated number of principal components to include in the model to deliver maximum AUC widely varies between 10 and 216 with a majority between 25 to 60. Given that there are totally 255 principal components and most models use under 60, we note that most of the data subsets must have colinearity between significant number of their features.  

### Naive Bayes Regression Models

1.  We now develop Naive Bayes regression models for the same 50 subsets and use the *train-val-test* strategy as for the Penalised Logistic Regression models.

```{r multiple-naive-bayes-models, warning=FALSE}
test.auc = 0
rows = 0

for (subset in subsets) {
  dat = train[train$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame(.)
  y = dat$target
  dat = subset(dat, select = -c(target, wheezy.copper.turtle.magic))
  m = nrow(dat)
  # Combining peer subsets from the train and test datasets
  dat = test[test$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame(.) %>% 
    subset(., select = -wheezy.copper.turtle.magic) %>% rbind(dat, .)
  
  pca.out = prcomp(dat) 
  dat = pca.out$x[1:m, ]
  
  set.seed(subset)
  indx = split(sample(1:nrow(dat)), f = c(rep("train", 6), rep("val", 2), rep("test", 2)))
  
  auc = rep(0, length(comps))   # Store the val-set auc for models with candidate number of principal components
  j = 1
  for (comp in comps) {
    model = naiveBayes(dat[indx$train, 1:comp], y[indx$train])
    
    prob = predict(model, newdata = dat[indx$val, 1:comp], type = "raw")[, "1"]
    pred = prediction(prob, y[indx$val])
    auc[j] = performance(pred, "auc")@y.values %>% unlist()
    j = j + 1
  }
  
  best.comp = which.max(auc) %>% comps[.]
  xval.comps[xval.comps[, "subset"] == subset, "X-val number of prinicpal Components"] = best.comp  
  
  set.seed(best.comp)
  model = naiveBayes(dat[-indx$test, 1:best.comp], y[-indx$test])
  
  prob = predict(model, newdata = dat[indx$test, 1:best.comp], type = "raw")[, "1"]
  pred = prediction(prob, y[indx$test])
  perf = performance(pred, "auc")
  perf = perf@y.values %>% unlist() %>% round(., 4)

  model.perf[model.perf[, "subset"] == subset, "NB"] = perf

  test.auc = test.auc + nrow(dat)*perf
  rows = rows + nrow(dat)
} 

writeLines("")
print(paste("The weighted estimated test AUC for the 50 subsets of train =", test.auc/rows))
print("The number of x-validated number of principal components used to build the 50 models are")
table(xval.comps[, "X-val number of prinicpal Components"])

```

*Observations*

1.  The collection of Naive Bayes models slightly outperforms (0.7901) the collection of the Penalised Regression models.
2.  For the Naive Bayes models, the cross validated number of principal components used vary between 10 and 141 with the majority between 25 and 50.

### QDA Regression Models

1.  Finally, we develop Quadratic Discriminant Analysis models for the same 50 subsets and use the *train-val-test* strategy as for the Penalised Logistic Regression models.

```{r multiple-qda-models, warning=FALSE}
test.auc = 0
rows = 0

for (subset in subsets) {
  dat = train[train$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame(.)
  y = dat$target
  dat = subset(dat, select = -c(target, wheezy.copper.turtle.magic))
  m = nrow(dat)
  # Combining peer subsets from the train and test datasets
  dat = test[test$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame(.) %>% 
    subset(., select = -wheezy.copper.turtle.magic) %>% rbind(dat, .)
  
  pca.out = prcomp(dat) 
  dat = pca.out$x[1:m, ]
  
  set.seed(subset)
  indx = split(sample(1:nrow(dat)), f = c(rep("train", 6), rep("val", 2), rep("test", 2)))
  
  auc = rep(0, length(comps))   # Store the val-set auc for models with candidate number of principal components
  j = 1
  for (comp in comps) {
    model = qda(dat[indx$train, 1:comp], y[indx$train])
    
    prob = predict(model, newdata = dat[indx$val, 1:comp])$posterior[, "1"]
    pred = prediction(prob, y[indx$val])
    auc[j] = performance(pred, "auc")@y.values %>% unlist()
    j = j + 1
  }
  
  best.comp = which.max(auc) %>% comps[.]
  xval.comps[xval.comps[, "subset"] == subset, "X-val number of prinicpal Components"] = best.comp
  
  set.seed(best.comp)
  model = qda(dat[-indx$test, 1:best.comp], y[-indx$test])
  
  prob = predict(model, newdata = dat[indx$test, 1:best.comp])$posterior[, "1"]
  pred = prediction(prob, y[indx$test])
  perf = performance(pred, "auc")
  perf = perf@y.values %>% unlist() %>% round(., 4)

  model.perf[model.perf[, "subset"] == subset, "QDA"] = perf

  test.auc = test.auc + nrow(dat)*perf
  rows = rows + nrow(dat)
} 

writeLines("")
print(paste("The weighted estimated test AUC for the 50 subsets of train =", test.auc/rows))
print("The number of x-validated number of principal components used to build the 50 models are")
table(xval.comps[, "X-val number of prinicpal Components"])

```

*Observations*

1.  The QDA requires a cross-validation interval for the principal components to use to be much smaller than that for the other two models. We therefore change the interval from 10-235 to 10-130 and re-develop the QDA models.

### QDA Regression Models - Redux

```{r multiple-qda-models-redux, warning=FALSE}
comps = seq(10, 130, length.out = 25) %>% floor()
test.auc = 0
rows = 0

for (subset in subsets) {
  dat = train[train$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame(.)
  y = dat$target
  dat = subset(dat, select = -c(target, wheezy.copper.turtle.magic))
  m = nrow(dat)
  # Combining peer subsets from the train and test datasets
  dat = test[test$wheezy.copper.turtle.magic == subset, ] %>% as.data.frame(.) %>% 
    subset(., select = -wheezy.copper.turtle.magic) %>% rbind(dat, .)
  
  pca.out = prcomp(dat) 
  dat = pca.out$x[1:m, ]
  
  set.seed(subset)
  indx = split(sample(1:nrow(dat)), f = c(rep("train", 6), rep("val", 2), rep("test", 2)))
  
  auc = rep(0, length(comps))   # Store the val-set auc for models with candidate number of principal components
  j = 1
  for (comp in comps) {
    model = qda(dat[indx$train, 1:comp], y[indx$train])
    
    prob = predict(model, newdata = dat[indx$val, 1:comp])$posterior[, "1"]
    pred = prediction(prob, y[indx$val])
    auc[j] = performance(pred, "auc")@y.values %>% unlist()
    j = j + 1
  }
  
  best.comp = which.max(auc) %>% comps[.]
  xval.comps[xval.comps[, "subset"] == subset, "X-val number of prinicpal Components"] = best.comp
  
  set.seed(best.comp)
  
  model = qda(dat[-indx$test, 1:best.comp], y[-indx$test])
  
  prob = predict(model, newdata = dat[indx$test, 1:best.comp])$posterior[, "1"]
  pred = prediction(prob, y[indx$test])
  perf = performance(pred, "auc")
  perf = perf@y.values %>% unlist() %>% round(., 4)

  model.perf[model.perf[, "subset"] == subset, "QDA"] = perf

  test.auc = test.auc + nrow(dat)*perf
  rows = rows + nrow(dat)
} 

writeLines("")
print(paste("The weighted estimated test AUC for the 50 subsets of train =", test.auc/rows))
print("The number of x-validated number of principal components used to build the 50 models are")
table(xval.comps[, "X-val number of prinicpal Components"])

```

*Observations*

1.  The QDA models overall vastly outperform both the Penalised Logistic Regression models and the Naive Bayes models. Interestingly, the cross-validated number of components used vary in a very narrow range from 25 to 50.
2.  We must investigate if the QDA model for every subset outperforms its peer Penalised Logistic Regression and Naive Bayes model.

```{r best-model}
model.perf = cbind(model.perf, best.model = apply(model.perf[, 2:4], 1, function(x){which.max(x) %>% names()}))
model.perf[, "best.model"]

```

*Observations*

1.  The QDA is the preferred model to use for all subsets of the *train* (and *test*) dataset when making the predictions.

